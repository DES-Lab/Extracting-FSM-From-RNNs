{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python368jvsc74a57bd02ab5e152fcf54322d6bbdd14ee60841ff95c5546cc9d5585d8c09cbd67c517cd",
   "display_name": "Python 3.6.8 64-bit ('venv')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from aalpy.SULs import MealySUL\n",
    "from aalpy.oracles import StatePrefixEqOracle\n",
    "\n",
    "from Applications import label_sequences_with_correct_model\n",
    "from DataProcessing import get_coffee_machine, generate_data_from_automaton, split_train_validation, tokenize\n",
    "from RNNClassifier import RNNClassifier\n",
    "from TrainAndExtract import extract_finite_state_transducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coffee machine FSM used for data generation\n",
    "ground_truth_model = get_coffee_machine()\n",
    "\n",
    "# Get input and output alphabet \n",
    "input_al = ground_truth_model.get_input_alphabet()\n",
    "output_al = {output for state in ground_truth_model.states for output in state.output_fun.values()}\n",
    "\n",
    "# Create the small initial training set\n",
    "train_seq, train_labels = generate_data_from_automaton(ground_truth_model, input_al,\n",
    "                                                       num_examples=1000, lens=(3, 6, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a number of RNNs that will be used for learning-based testing\n",
    "# Each NN will be trained with the training data set.\n",
    "# Models will be mined from all RNNs and cross checked for conformance.\n",
    "# Cases of non-conformance are added to the training set.\n",
    "NUM_RNNs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning/extraction round: 1\nStarting training of RNN 0\nStarting training of RNN 1\nStarting training of RNN 2\nStarting training of RNN 3\nStarting extraction of the automaton from RNN 0\nStarting extraction of the automaton from RNN 1\nStarting extraction of the automaton from RNN 2\nStarting extraction of the automaton from RNN 3\nAdding 260 new examples to training data.\nSize of training data: 1262\nLearning/extraction round: 2\nStarting training of RNN 0\nStarting training of RNN 1\nStarting training of RNN 2\nStarting training of RNN 3\nStarting extraction of the automaton from RNN 0\nStarting extraction of the automaton from RNN 1\nStarting extraction of the automaton from RNN 2\nStarting extraction of the automaton from RNN 3\nAdding 308 new examples to training data.\nSize of training data: 1570\nLearning/extraction round: 3\nStarting training of RNN 0\nStarting training of RNN 1\nStarting training of RNN 2\nStarting training of RNN 3\nStarting extraction of the automaton from RNN 0\nStarting extraction of the automaton from RNN 1\nStarting extraction of the automaton from RNN 2\nStarting extraction of the automaton from RNN 3\nAdding 464 new examples to training data.\nSize of training data: 2034\nLearning/extraction round: 4\nStarting training of RNN 0\nStarting training of RNN 1\nStarting training of RNN 2\nStarting training of RNN 3\nStarting extraction of the automaton from RNN 0\nStarting extraction of the automaton from RNN 1\nStarting extraction of the automaton from RNN 2\nStarting extraction of the automaton from RNN 3\nAdding 324 new examples to training data.\nSize of training data: 2358\nLearning/extraction round: 5\nStarting training of RNN 0\nStarting training of RNN 1\nStarting training of RNN 2\nStarting training of RNN 3\nStarting extraction of the automaton from RNN 0\nStarting extraction of the automaton from RNN 1\nStarting extraction of the automaton from RNN 2\nStarting extraction of the automaton from RNN 3\nAdding 141 new examples to training data.\nSize of training data: 2499\nLearning/extraction round: 6\nStarting training of RNN 0\nStarting training of RNN 1\nStarting training of RNN 2\nStarting training of RNN 3\nStarting extraction of the automaton from RNN 0\nStarting extraction of the automaton from RNN 1\nStarting extraction of the automaton from RNN 2\nStarting extraction of the automaton from RNN 3\nSize of automata 0: 6\nSize of automata 1: 6\nSize of automata 2: 6\nSize of automata 3: 6\ndigraph learnedModel {\ns0 [label=s0];\ns1 [label=s1];\ns2 [label=s2];\ns3 [label=s3];\ns4 [label=s4];\ns5 [label=s5];\ns0 -> s0  [label=\"clean/check\"];\ns0 -> s5  [label=\"pod/check\"];\ns0 -> s2  [label=\"water/check\"];\ns0 -> s1  [label=\"button/star\"];\ns1 -> s1  [label=\"clean/star\"];\ns1 -> s1  [label=\"pod/star\"];\ns1 -> s1  [label=\"water/star\"];\ns1 -> s1  [label=\"button/star\"];\ns2 -> s0  [label=\"clean/check\"];\ns2 -> s3  [label=\"pod/check\"];\ns2 -> s2  [label=\"water/check\"];\ns2 -> s1  [label=\"button/star\"];\ns3 -> s0  [label=\"clean/check\"];\ns3 -> s3  [label=\"pod/check\"];\ns3 -> s3  [label=\"water/check\"];\ns3 -> s4  [label=\"button/coffee\"];\ns4 -> s0  [label=\"clean/check\"];\ns4 -> s1  [label=\"pod/coffee\"];\ns4 -> s1  [label=\"water/coffee\"];\ns4 -> s1  [label=\"button/coffee\"];\ns5 -> s0  [label=\"clean/check\"];\ns5 -> s5  [label=\"pod/check\"];\ns5 -> s3  [label=\"water/check\"];\ns5 -> s1  [label=\"button/star\"];\n__start0 [label=\"\", shape=none];\n__start0 -> s0  [label=\"\"];\n}"
     ]
    }
   ],
   "source": [
    "# While the input-output behaviour of all trained neural networks is different\n",
    "iteration = 0\n",
    "while True:\n",
    "    iteration += 1\n",
    "    print(f'Learning/extraction round: {iteration}')\n",
    "\n",
    "    trained_networks = []\n",
    "\n",
    "    x_train, y_train, x_test, y_test = split_train_validation(train_seq, train_labels, 0.8, uniform=True)\n",
    "\n",
    "    # Train all neural networks with same parameters\n",
    "    for i in range(NUM_RNNs):\n",
    "        rnn = RNNClassifier(input_al, output_dim=len(output_al), num_layers=2, hidden_dim=40,\n",
    "                            x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test,\n",
    "                            batch_size=32, nn_type='GRU')\n",
    "        print(f'Starting training of RNN {i}')\n",
    "        rnn.train(epochs=150, stop_acc=1.0, stop_epochs=3, verbose=False)\n",
    "        trained_networks.append(rnn)\n",
    "\n",
    "    learned_automatons = []\n",
    "\n",
    "    # Extract automaton for each neural network\n",
    "    for i, rnn in enumerate(trained_networks):\n",
    "        print(f'Starting extraction of the automaton from RNN {i}')\n",
    "        learned_automaton = extract_finite_state_transducer(rnn, input_al, output_al, max_learning_rounds=6,\n",
    "                                                            print_level=0)\n",
    "        learned_automatons.append(learned_automaton)\n",
    "\n",
    "    learned_automatons.sort(key=lambda x: len(x.states), reverse=True)\n",
    "\n",
    "    # Select one automaton as a basis for conformance-checking. You can also do conformance checking with all pairs\n",
    "    # of learned automata.\n",
    "\n",
    "    base_sul = MealySUL(learned_automatons[0])\n",
    "\n",
    "    # Select the eq. oracle\n",
    "\n",
    "    eq_oracle = StatePrefixEqOracle(input_al, base_sul, walks_per_state=100, walk_len=50)\n",
    "\n",
    "    cex_set = set()\n",
    "\n",
    "    # Try to find cases of non-conformance between learned automatons.\n",
    "    for la in learned_automatons[1:]:\n",
    "        for i in range(200):\n",
    "            cex = eq_oracle.find_cex(la)\n",
    "            if cex:\n",
    "                cex_set.add(tuple(cex))\n",
    "\n",
    "    # If there were no counterexamples between any learned automata, we end the procedure\n",
    "    if not cex_set:\n",
    "        for i, la in enumerate(learned_automatons):\n",
    "            print(f'Size of automata {i}: {len(la.states)}')\n",
    "        print(learned_automatons[-1])\n",
    "        print('No counterexamples between extracted automata found.')\n",
    "        break\n",
    "\n",
    "    # Ask ground truth model for correct labels\n",
    "    new_x, new_y = label_sequences_with_correct_model(ground_truth_model, cex_set)\n",
    "\n",
    "    print(f'Adding {len(cex_set)} new examples to training data.')\n",
    "    new_x = tokenize(new_x, input_al)\n",
    "    new_y = tokenize(new_y, output_al)\n",
    "\n",
    "    train_seq.extend(new_x)\n",
    "    train_labels.extend(new_y)\n",
    "    print(f'Size of training data: {len(train_seq)}')"
   ]
  }
 ]
}